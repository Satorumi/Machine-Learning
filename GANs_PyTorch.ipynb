{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANs_PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOdKiLB+P7qnzm8x9bsfABi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satorumi/Machine-Learning/blob/main/GANs_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yjkf-PKjZvE"
      },
      "source": [
        "## GANs - Generative Adversarial Networks\n",
        "\n",
        "There are two neural networks: a *Generator* and a *Discriminator*. The generator generates a \"fake\" sample given a random vector/matrix, and the discriminator attempts to detect whether a given sample is \"real\" (picked from the training data) or \"fake\" (generated by the generator). Training happens in tandem: we train the discriminator for a few epochs, then train the generator for a few epochs, and repeat. This way both the generator and the discriminator get better at doing their jobs. \n",
        "\n",
        "GANs however, can be notoriously difficult to train, and are extremely sensitive to hyperparameters, activation functions and regularization.\n",
        "ref: [Generating Images using Generative Adversarial Networks](https://jovian.ai/aakashns/06b-anime-dcgan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U_tPuT5eUJ5"
      },
      "source": [
        "####Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwniJl1GKf5e"
      },
      "source": [
        "!pip install opendatasets --upgrade --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTk4AeLZK25Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "4f9f4f82-7a4b-424f-bf42-0a1b07ff5339"
      },
      "source": [
        "import os\n",
        "import opendatasets as od\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils\n",
        "import numpy as np\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e4db4049d807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopendatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'opendatasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBk6pnO5ec9d"
      },
      "source": [
        "####Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEZ3Qby3LFpU"
      },
      "source": [
        "dataset_url =\n",
        "# use opendatasets to import kaggle datset\n",
        "od.download(dataset_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJyfdh4DLFvb"
      },
      "source": [
        "data_dir = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7POUg4M8gTQH"
      },
      "source": [
        "###Preprocessing Dataset\n",
        "Load this dataset using the ImageFolder class from torchvision. Resize and normalize images' pixels value. Then, create a dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuF8WRPTh3Ks"
      },
      "source": [
        "Define device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHpp8pBLh5nn"
      },
      "source": [
        "device = 'gpu' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRi8Mo0iRIaS"
      },
      "source": [
        "#  define the image size which all image will have\n",
        "image_size = 64\n",
        "batch_size = 128\n",
        "data_stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU2UNFcF_Pkk"
      },
      "source": [
        "Load and transform data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I-lexnZetbj"
      },
      "source": [
        "transformations = transforms.Compose([transform=transforms.Compose([\n",
        "                    # resize all image to the same size                                              \n",
        "                    transforms.Resize(image_size),\n",
        "                    transforms.CenterCrop(image_size),\n",
        "                    transforms.ToTensor(), # convert to tensor obj\n",
        "                    # normalize pixel with computed stats\n",
        "                    transforms.Normalize(*data_stats)\n",
        "                                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m25pwpz5g35r"
      },
      "source": [
        "dataset = ImageFolder(data_dir, transforms=transformations)\n",
        "# using torch.utils.data.DataLoader\n",
        "dataloader = Dataloader(dataset, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUaI1slViiWO"
      },
      "source": [
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=10,\n",
        "    num_workers=1,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# compute the standard deviation and mean og img\n",
        "mean, std = 0., 0.\n",
        "nb_samples = 0.\n",
        "\n",
        "for data in train_dataloader:\n",
        "    batch_samples = data.size(0)\n",
        "    data = data.view(batch_samples, data.size(1), -1)\n",
        "    mean += data.mean(2).sum(0)\n",
        "    std += data.std(2).sum(0)\n",
        "    nb_samples += batch_samples\n",
        "\n",
        "mean /= nb_samples\n",
        "std /= nb_samples\n",
        "stats = mean, std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYtRndhsSLxU"
      },
      "source": [
        "Explore And Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3LwSn5ngY2p"
      },
      "source": [
        "# display a batch of img\n",
        "batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "# denormalize and display img\n",
        "plt.imshow(np.transpose(utils.make_grid(real_batch[0].to(device)[:64], \n",
        "                        padding=2, normalize=True).cpu(),(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQvloM-Hh0T1"
      },
      "source": [
        "###Buid GANs Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-OJyhWOckWJ"
      },
      "source": [
        "####Discriminator Model\n",
        "The discriminator takes an image as input, and tries to classify it as \"real\" or \"generated\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSRHFqqocxY9"
      },
      "source": [
        "# Define Discriminator model with Convolutional Network\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.classification = nn.Sequential(\n",
        "        \n",
        "        # input layer with 3 x 64 x 64\n",
        "        nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False), # out 64\n",
        "        nn.BatchNorm2d(64), # add a batch normalization 64\n",
        "        # leaky relu level layer return very small output for negative value\n",
        "        nn.LeakyReLU(0.2, inplace=True), # size: 64 x 32 x 32\n",
        "\n",
        "        nn.Conv2d(64, 128, 4, 2, 1, False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(0.2, inplace=True), # size: 128 x 16 x 16\n",
        "\n",
        "        nn.Conv2d(128, 256, 4, 2, 1, False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.LeakyReLU(0.2, inplace=True), # size: 256 x 8 x 8\n",
        "\n",
        "        nn.Conv2d(256, 512, 4, 2, 1, False),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.LeakyReLU(0.2, inplace=True), # size: 512 x 4 x 4\n",
        "\n",
        "        nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=2 False)\n",
        "        nn.Flatten(),\n",
        "        \n",
        "        # use sigmoid activation func for binary classification\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.classification(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWeXeKO9hkJn"
      },
      "source": [
        "discriminator_model = Discriminator().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k7hpAEbfwSh"
      },
      "source": [
        "####Generator Model\n",
        "The input to the generator is typically a vector or a matrix of random numbers (latent tensor) which is used as a seed for generating an image. The generator will convert a latent tensor of shape (128, 1, 1) into an image tensor of shape 3 x 28 x 28. To achive this, we'll use the ConvTranspose2d layer from PyTorch, which is performs to as a transposed convolution (or deconvolution). [Learn more](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md#transposed-convolution-animations)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxFR5UN8mWvO"
      },
      "source": [
        "latent_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltcMsHo_fyCW"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    \n",
        "    self.generate = nn.Sequential(\n",
        "        # input size: latent x 1 x 1\n",
        "        nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(inplace=True) # size: 512 x 4 x 4\n",
        "\n",
        "        nn.ConvTranspose2d(512, 256, 4, 2, 1, False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(True), # size 256 x 8 x 8\n",
        "\n",
        "        nn.ConvTranspose2d(256, 128, 4, 2, 1, False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(True), # size 128 x 16 x 16\n",
        "\n",
        "        nn.ConvTranspose2d(128, 64, 4, 2, 1, False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True), # size 64 x 32 x 32\n",
        "\n",
        "        # output layer: size 3 x 64 x 64\n",
        "        nn.ConvTranspose2d(64, 3, 4, 2, 1, False),\n",
        "        nn.Tanh() # activation function range val from -1 -> 1\n",
        "    )\n",
        "\n",
        "  def forward(X):\n",
        "    return self.generate(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6RuT5iZrgFW"
      },
      "source": [
        "generator_model = Generator().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD9E1Sx3qgTI"
      },
      "source": [
        "Test generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZGBTn2kqi_P"
      },
      "source": [
        "# use a fixed latent to see how the generated images \n",
        "# develop as we train the model\n",
        "fixed_latent = torch.randn(batch_size, latent_size, 1, 1, device=device) # random latent tensors\n",
        "\n",
        "generated_img = generator_model(test_x)\n",
        "print(generated_img.shape)\n",
        "plt.imshow(np.transpose(utils.make_grid(generated_img[0].to(device)[:64], \n",
        "                        padding=2, normalize=True).cpu(),(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwMZTaAWkSan"
      },
      "source": [
        "###Training Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3EqZKJJbki7"
      },
      "source": [
        "Define loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1P7GqlINejx"
      },
      "source": [
        "# define the loss function\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "d_optimizer = torch.optim.Adam(discriminator_model.parameters(), learning_rate)\n",
        "g_optimizer = torch.optim.Adam(generator_model.parameters(), learning_rate)\n",
        "\n",
        "# encoded binary classification\n",
        "real_classified = 1 \n",
        "generated_classified = 0 # represent generated img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LD0FzNv79OF"
      },
      "source": [
        "\n",
        "\n",
        "####Steps involved in training the discriminator.\n",
        "\n",
        "We expect the discriminator to output 1 if the image was picked from the real MNIST dataset, and 0 if it was generated using the generator network.\n",
        "\n",
        "1. We first pass a batch of real images, and compute the loss, setting the target labels to 1.\n",
        "\n",
        "2. Then we pass a batch of fake images (generated using the generator) pass them into the discriminator, and compute the loss, setting the target labels to 0.\n",
        "\n",
        "3. Finally we add the two losses and use the overall loss to perform gradient descent to adjust the weights of the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vePqL4FkUMc"
      },
      "source": [
        "def train_discriminator(images, discriminator_model, generator_model, d_optimizer, g_optimizer):\n",
        "  \n",
        "  real_classified = torch.ones(batch_size, 1).to(device) # 1 represent real img\n",
        "  generated_classified = torch.zeros(batch_size, 0).to(device) # 0 represent generated img\n",
        "  \n",
        "  # passin real imgs in discrimininator model and get classfied results\n",
        "  real_classification = discrimininator_model(images) \n",
        "\n",
        "  # compute the loss for real imgs\n",
        "  real_loss = F.binary_cross_entropy(real_classification, real_classified)\n",
        "\n",
        "  # generate a latent and img from generator\n",
        "  z = torch.randn(batch_size, latent_size).to(device)\n",
        "  generated_img = generator_model(z)\n",
        "  # classified the generated img\n",
        "  generator_classification = discriminator_model(generated_img)\n",
        "  # compute the loss for generate imgs\n",
        "  generate_loss = F.binary_cross_entropy(generator_classification, generated_classified)\n",
        "\n",
        "  # combine losses -> overall loss\n",
        "  loss = real_loss + generate_loss\n",
        "\n",
        "  ## Backpropagation\n",
        "  # reset gradients\n",
        "  d_optimizer.no_grad()\n",
        "  g_optimizer.no_grad()\n",
        "\n",
        "  # compute gradient\n",
        "  loss.backward()\n",
        "\n",
        "  d_optimizer.step()\n",
        "\n",
        "  return real_loss, generate_loss, loss.item()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifiNJ295-khT"
      },
      "source": [
        "use the binary cross entropy loss function to evaluate the loss of binary classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKNZfyXA8hUL"
      },
      "source": [
        "####Steps invlove in Generator Training\n",
        "1. We generate a batch of images using the generator, pass the into the discriminator.\n",
        "\n",
        "2. We calculate the loss by setting the target labels to 1 i.e. real. We do this because the generator's objective is to \"fool\" the discriminator.\n",
        "\n",
        "3. We use the loss to perform gradient descent i.e. change the weights of the generator, so it gets better at generating real-like images to \"fool\" the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuUUE2qayCQl"
      },
      "source": [
        "def train_generator(discriminator_model, generator_model, loss_function, d_optimizer, g_optimizer):\n",
        "  real_classified = torch.ones(batch_size, 1).to(device) # 1 represent real img\n",
        "\n",
        "  z = torch.randn(batch_size, latent_size).to(device) # a random lantent\n",
        "  # generated based on z\n",
        "  generated_img = generator_model(z)\n",
        "  # classified the generated img\n",
        "  generator_classification = discriminator_model(generated_img)\n",
        "  # compare with real img to get the loss as we want the fake to be classified aas real\n",
        "  generator_loss = loss_function(generator_classification, real_classified)\n",
        "\n",
        "  ## Backpropagation\n",
        "  # reset gradients\n",
        "  d_optimizer.no_grad()\n",
        "  g_optimizer.no_grad()\n",
        "\n",
        "  # compute gradient\n",
        "  generator_loss.backward()\n",
        "\n",
        "  g_optimizer.step()\n",
        "\n",
        "  return generator_loss.item(), generated_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-wns7SSzuuw"
      },
      "source": [
        "####Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS7F-0btNsVM"
      },
      "source": [
        "learning_rate = 1e-2\n",
        "batch_size = 128\n",
        "img_size = 64\n",
        "epochs = 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFbTBIjI4kX1"
      },
      "source": [
        "num_batches = len(dataloader)\n",
        "data_size = len(dataloader.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtI4MJy83vD5"
      },
      "source": [
        "# tracking the process and storing value\n",
        "generated_imgs = []\n",
        "generator_losses = []\n",
        "discriminator_losses = []\n",
        "real_scores = []\n",
        "fake_scores = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pxDGJz0zwkd"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  print(f'Epoch {epoch+1}\\n-------------------------------')\n",
        "  total_gloss, total_dloss = 0., 0.\n",
        "  total_realscores, total_fakescores = 0., 0.\n",
        "\n",
        "  for batch, (images, _ ) in enumerate(data_loader):\n",
        "    # reshape all the images in a batch\n",
        "    images = images.reshape(batch_size, -1).to(device)\n",
        "    \n",
        "    # train the discrimninator with batch of imgs\n",
        "    real_score, fake_score, d_loss = train_discriminator(images, discriminator_model, generator_model, \n",
        "                                    loss_function, d_optimizer, g_optimizer)\n",
        "    \n",
        "    # train the generator\n",
        "    generator_loss, generated_img = train_generator(discriminator_model, generator_model, \n",
        "                    loss_function, d_optimizer, g_optimizer)\n",
        "    \n",
        "    # print result for every 500 min-batches\n",
        "    if batch % 500 == 0:\n",
        "      cur_batch = (batch+1) * len(images)\n",
        "      print(f\"Discriminator Loss: {d_loss.item():>4f}, Generator Loss: {generator_loss.item():>4f}  [{cur_batch:>5d}/{data_size:>5d}]\")\n",
        "      \n",
        "  \n",
        "  # compute the mean stats for each epoch\n",
        "  generator_losses.append((total_gloss/num_batches).item())\n",
        "  discriminator_losses.append((total_dloss/num_batches).item())\n",
        "  real_scores.append((total_realscores/num_batches).item())\n",
        "  fake_scores.append((total_fakescores/num_batches).item())\n",
        "\n",
        "  generated_imgs.append(generated_img) # save a sample of generator img\n",
        "  \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9CXZvUOBFp5"
      },
      "source": [
        "#### Full Training Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe0PAonKBkEX"
      },
      "source": [
        "def fit(epochs, lr, dataloader):\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  generated_imgs = []\n",
        "  generator_losses = []\n",
        "  discriminator_losses = []\n",
        "  real_scores = []\n",
        "  fake_scores = []\n",
        "\n",
        "  # define optimizer\n",
        "  d_optimizer = torch.optim.Adam(discriminator_model.parameters(), learning_rate)\n",
        "  g_optimizer = torch.optim.Adam(generator_model.parameters(), learning_rate)\n",
        "\n",
        "  ## training process\n",
        "  for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}\\n-------------------------------')\n",
        "\n",
        "  total_gloss, total_dloss = 0., 0.\n",
        "  total_realscores, total_fakescores = 0., 0.\n",
        "\n",
        "  for batch, (images, _ ) in enumerate(data_loader):\n",
        "    # reshape all the images in a batch\n",
        "    images = images.reshape(batch_size, -1).to(device)\n",
        "    \n",
        "    # train the discrimninator and generator model\n",
        "    real_score, fake_score, d_loss = train_discriminator(images, discriminator_model, generator_model, \n",
        "                                    loss_function, d_optimizer, g_optimizer):\n",
        "    generator_loss, generated_img = train_generator(discriminator_model, generator_model, \n",
        "                    loss_function, d_optimizer, g_optimizer)\n",
        "    \n",
        "    # print result for every 500 min-batches\n",
        "    if batch % 500 == 0:\n",
        "      cur_batch = (batch+1) * len(images)\n",
        "      print(f\"Discriminator Loss: {d_loss.item():>4f}, Generator Loss: {generator_loss.item():>4f}  [{cur_batch:>5d}/{data_size:>5d}]\")\n",
        "      \n",
        "     \n",
        "  \n",
        "  # compute the mean stats for each epoch\n",
        "  generator_losses.append((total_gloss/num_batches).item())\n",
        "  discriminator_losses.append((total_dloss/num_batches).item())\n",
        "  real_scores.append((total_realscores/num_batches).item())\n",
        "  fake_scores.append((total_fakescores/num_batches).item())\n",
        "\n",
        "  generated_imgs.append(generated_img) # save a sample of generator img\n",
        "\n",
        "  return generator_losses, discriminator_losses, real_scores, fake_scores, generated_imgs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHzO-XaXTnCD"
      },
      "source": [
        "history = fit(epochs, learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoZ3wq5ZTiNN"
      },
      "source": [
        "###Visualize result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grflvSLmW-vY"
      },
      "source": [
        "####Losses and Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v3iHjNMUzjT"
      },
      "source": [
        "generator_losses, discriminator_losses, real_scores, fake_scores, generated_imgs = history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOTfTPe5ThZz"
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.plot(range(epochs), generator_losses, 'r-x')\n",
        "plt.plot(range(epochs), generator_losses, 'r-x')\n",
        "plt.legend(['Discriminator', 'Generator'], loc='upper right')\n",
        "plt.title('Discriminator and Genrator Losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Rate')\n",
        "ply.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdzgEzwPWZ0t"
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.plot(range(epochs), real_scores, 'r-x')\n",
        "plt.plot(range(epochs), fake_scores, 'r-x')\n",
        "plt.legend(['Real Classification', 'Generated Classification'], loc='upper right')\n",
        "plt.title('Discriminator Classification Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Rate')\n",
        "ply.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QynbbQfeW9Zq"
      },
      "source": [
        "####Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nVfu7lgZdi0"
      },
      "source": [
        "Genrated Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6TakrH2XD8M"
      },
      "source": [
        "# display a video of generated imgs\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "plt.axis('off')\n",
        "# using matplotlib.animation for denormalized images\n",
        "images = [[plt.imshow(np.tranpose(img, (1,2,0)), animated=True)] for img in generated_imgs]\n",
        "animated = animation.ArtistAnimation(fig, images, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(animated.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zEgz0qgZbvo"
      },
      "source": [
        "Final Results and Comparision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_wWGxhBZhij"
      },
      "source": [
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "# display real images\n",
        "batch, (images, _ ) = next(iter(dataloader)) # get a batch from dataloader\n",
        "# denormalize the first 64 imgs\n",
        "images = torchvision.utils.make_grid(images).to(device)[:64], padding=5, normalize=True)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.axis('off')\n",
        "plt.title('Real Images')\n",
        "plt.imshow(np.tranpose(images, (1,2,0))) \n",
        "\n",
        "# display generated images\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.axis('off')\n",
        "plt.title('Generated Images')\n",
        "# display the last img from generated list\n",
        "plt.imshow(np.tranpose(generated_imgs[-1], (1,2,0)) # de normalize\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv7YvSgAz9dg"
      },
      "source": [
        "### Save Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1MMsNYk--V9"
      },
      "source": [
        "torch.save(generator_model.state_dict(), 'generator_model.pth')\n",
        "torch.save(discriminator_model.state_dict(), 'discriminator_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}