{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenCV_Projects.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPnqDEkK/tUBQNkUPrVoYpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satorumi/Machine-Learning/blob/main/OpenCV_Projects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpNmXU7oWpzV"
      },
      "source": [
        "##**Attendance with Face Recognition Project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnTR7itj40sl",
        "outputId": "1b6bd735-3a1c-415d-a4c4-3c80e58c5014"
      },
      "source": [
        "!pip install face-recognition"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting face-recognition\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face-recognition) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face-recognition) (1.19.5)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
            "\u001b[K     |████████████████████████████████| 100.2MB 91kB/s \n",
            "\u001b[?25hRequirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face-recognition) (19.18.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face-recognition) (7.1.2)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566184 sha256=67d0509c042a25d0aca0666b47505998e0728f84a59c64bf91bfdee5b968609d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "E3sH5vQn9FJm",
        "outputId": "c39e81e7-cd3a-4a08-f7bf-39293600708b"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-57d92674-4765-438d-bffb-093c4ca5927c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-57d92674-4765-438d-bffb-093c4ca5927c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Harry Styles.jpeg to Harry Styles.jpeg\n",
            "Saving Taylor Swift.jpg to Taylor Swift (1).jpg\n",
            "User uploaded file \"Harry Styles.jpeg\" with length 80962 bytes\n",
            "User uploaded file \"Taylor Swift.jpg\" with length 739889 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltkrIAny3to8"
      },
      "source": [
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import dlib\n",
        "import face_recognition as fr\n",
        "import os\n",
        "from datetime import datetime\n",
        "import csv"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-fSX77q6vfP"
      },
      "source": [
        "#####Using face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-HQzKUI7VFh"
      },
      "source": [
        "# import train and test image\n",
        "el_train = fr.load_image_file('el_train.jpg')\n",
        "el_test = fr.load_image_file('el_test.jpg')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOLePdhr6vHB"
      },
      "source": [
        "# convert color because the model only take in RGB color img\n",
        "rgb_train = cv.cvtColor(el_train, cv.COLOR_BGR2RGB)\n",
        "rgb_test = cv.cvtColor(el_test, cv.COLOR_BGR2RGB)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcHJ_oVF9wuh",
        "outputId": "3ec2ac23-6aab-4afc-8d6a-7e647c9a0fae"
      },
      "source": [
        "face_location = fr.face_locations(rgb_train)[0] # define face location\n",
        "w, x, y, h = face_loc\n",
        "encoded_train = fr.face_encodings(rgb_train)[0]\n",
        "encoded_test = fr.face_encodings(rgb_test)[0]\n",
        "  # draw the face location\n",
        "cv.rectangle(el_train, pt1=(x,y), pt2=(w, h), color=(0,0,255), thickness=2)\n",
        "cv2_imshow(el_train)\n",
        "face_location"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(139, 324, 325, 139)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_SYgo0u_MoL",
        "outputId": "b875f46b-3718-4c06-92d7-c4574b390ae5"
      },
      "source": [
        "# check if it recognized as the same person\n",
        "compare = fr.compare_faces([encoded_train], encoded_test) \n",
        "# compute distance between 2 img, the smaller distance, the more similar\n",
        "face_dis = fr.face_distance([encoded_train], encoded_test)\n",
        "compare, face_dis"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([True], array([0.2662764]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml0xkUNy_p0D"
      },
      "source": [
        "# put text\n",
        "cv.putText(el_train, (x+6, y-6), f'Comparision: {compare} / Face Distance{face_dis}',\\\n",
        "           cv.FONT_HERSHEY_COMPLEX, 1, (0,0,255), thickness=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkx4VSiTWRpS"
      },
      "source": [
        "###Attendance taking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVDUgC-sD8lT"
      },
      "source": [
        "# get all imgs from loal file\n",
        "path = 'C:\\Users\\lisa0\\OneDrive\\Images'\n",
        "img_paths = os.listdir(path)\n",
        "attendances = []\n",
        "images = []\n",
        "for img_path in img_paths:\n",
        "  img = cv.imread(f'{path}/{img_path}')\n",
        "  images.append(img)\n",
        "  attendances.append(os.path.splitext(cl)[0]) # get the name of the person"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nR6mGLQG4Lz"
      },
      "source": [
        "# define a func to encode imgs\n",
        "def encodding_img(imgs): # take in a list of img\n",
        "  encoded_imgs = []\n",
        "  for img in imgs:\n",
        "    rgb_img = cv.cvtColor(img, cv.COLOR_BGR2RGB)   \n",
        "    encoded_img = fr.face_encodings(rgb_img)[0]\n",
        "    encoded_imgs.append(encoded_img)\n",
        "  return encoded_imgs\n",
        "\n",
        "encoded_imgs = econdding_img(images) # pass in img list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig75fo4WIDKl"
      },
      "source": [
        "cap = cv.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "  _, frame = cap.read()\n",
        "  frameS = cv.resize(frame, (0,0), None, 0.25, 0.25) # scale down img\n",
        "  frameS = cv.cvtColor(frame, cv.COLOR_BGR2RGB) \n",
        "  # might be multiple faces\n",
        "  face_locations = fr.face_locations(frameS)[0] # define face location\n",
        "  w, x, y, h = face_locations * 4 # orginal location\n",
        "  encoded_face = fr.face_encodings(frameS)[0] # encode face\n",
        "\n",
        "  # loop through each face detected\n",
        "  for face, loc in zip(encoded_face, face_locations):\n",
        "    match = fr.compare_faces(encoded_imgs, face) # compare it with previous faces\n",
        "    dist = fr.face_distance(encoded_imgs, face) # compute distances\n",
        "    most_similar = attendances[np.argmin(dist)] # take the least dist and get the name\n",
        "    if match[np.argmin(dist)]: # check if match\n",
        "      # draw rectangle aroung detected face\n",
        "      cv.rectangle(el_train, pt1=(x,y), pt2=(w, h), color=(0,0,255), thickness=2)\n",
        "      # write the name of matched person \n",
        "      cv.putText(frameS, (x+6, y-6), most_similar,\n",
        "           cv.FONT_HERSHEY_COMPLEX, 1, (0,0,255), thickness=1)\n",
        "      \n",
        "      mark_attendance(most_similar) # use a function to mark attendance into csv file\n",
        "      \n",
        "  cv2_imshow(frameS)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVtKxWWLJ4Er"
      },
      "source": [
        "def mark_attendance(name):\n",
        "    # using panda to write in csv\n",
        "  # attendances_info = pd.read_csv('attendance.csv')\n",
        "  # if name not in attendances_info['name']:\n",
        "  #   time = datetime.now().strftime('%H-%S-%S %Y-%m-%d')\n",
        "  #   attendances_info.append({'name': name, 'attendance': time})\n",
        "  # attendances_info.to_csv('attendance.csv')\n",
        "\n",
        "  # Using csv library\n",
        "  with open('attendance.csv', 'r+') as f:\n",
        "    attendances_info = f.readlines()\n",
        "    attended = [] # already attended people\n",
        "    for info in attendances_info:\n",
        "      entry = info.split(',')\n",
        "      attended.append = entry[0]\n",
        "    if name not in attended:\n",
        "      time = datetime.now().strftime('%H-%S-%S %Y-%m-%d') # current time\n",
        "      writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "      writer.writerow([name, time]) # write new row\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz7OtIKsVdRb"
      },
      "source": [
        "## **QR-CODE AND BARCODE DETECTOR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHt8mRiKWCiT"
      },
      "source": [
        "import numpy as np\n",
        "from pyzbar.pyzbar import decode\n",
        "import cv2 as cv\n",
        "\n",
        "import imutils"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNeZHjZeXe3Y"
      },
      "source": [
        "# set up camera\n",
        "cap = cv.VideoCapture(0)\n",
        "camera_width, camera_height = 640, 480\n",
        "cap.set(3, camera_width)\n",
        "cap.set(4, camera_height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTmJ2suxYSwf"
      },
      "source": [
        "while True:\n",
        "  _, frame = cap.read()\n",
        "  barcodes = decode(frame) # get the barcode from img\n",
        "  for barcode in barcodes: # in case of multiple barcodes\n",
        "    # barcode data is a bytes object so we have to decode it to string\n",
        "    data = barcode.data.decode('utf-8')\n",
        "\n",
        "    # check if data is authorized\n",
        "    with open('authorized_data.csv', 'wb') as f: \n",
        "      authorized_data = f.read()\n",
        "      if data in authorized_data:\n",
        "        color = (0,255,0)\n",
        "      else:\n",
        "        color = (0,0,255)\n",
        "    # get the polygon shape of barcode\n",
        "    pts = np.array([barcode.polygon], np.int32).reshape((-1,1,2)) # reshape the array\n",
        "    data.polylines(frame, [pts], True, color, 5) # boundng polygon\n",
        "    x, y, w, h = barcode.rect # get the point of rectangle shape of qr-code\n",
        "    # write received data\n",
        "    cv.putText(frame, (x, y), data, cv.FONT_HERSHEY_COMPLEX, 1, color), 2)\n",
        "  cv.imshow(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJI9xBPsdY4-"
      },
      "source": [
        "####BarCode detector\n",
        "refs: \n",
        "- https://www.pyimagesearch.com/2014/11/24/detecting-barcodes-images-python-opencv/\n",
        "- https://laconicml.com/barcode-detection-opencv/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhKTGNZDibPp"
      },
      "source": [
        "cap = cv.VideoCapture(0)\n",
        "\n",
        "## Process the image to detect barcode\n",
        "while True:\n",
        "  _, img = cap.read()\n",
        "  gray_img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "  # find the gradient of the image with sobel\n",
        "  depth = cv.cv.CV_32F if imutils.is_cv2() else cv.CV_32F\n",
        "  xgrad = cv.sobel(gray_img, depth, dx=1, dy=0, ksize=-1)\n",
        "  ygrad = cv.sobel(gray_img, depth, dx=0, dy=1, ksize=-1)\n",
        "  # subtracting two grad\n",
        "  grad_img = cv.subtract(xgrad, ygrad) # result in high hor, low ver gradients\n",
        "  #Scales, calculates absolute values\n",
        "  grad_img = cv.convertScaleAbs(grad)\n",
        "\n",
        "  # blurr to smoothen the noise\n",
        "  blurred = cv.blurr(grad_img, ksize =(9,9))\n",
        "  # pixel in gradient > 255 -> 255, otherwise black (0)\n",
        "  _, thresh = cv.threshold(blurred, 255, 255, cv.THRESH_BINARY)\n",
        "\n",
        "  # apply a close kernel to the threshold img\n",
        "  # construct a closed kernel\n",
        "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (21, 7))\n",
        "  closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
        "  # erod and dilation to remove excess white area \n",
        "  # and close the black space arounf bar code\n",
        "  closed = cv2.erode(closed, None, iterations = 4)\n",
        "  closed = cv2.dilate(closed, None, iterations = 4)\n",
        "\n",
        "  # find contours and draw it\n",
        "  contours, _ = cv.findcountours(closed.copy()), cv.RETR_EXTERNAL, CV.CHAIN_APPROX_SIMPLE)\n",
        "  contours = imutils.grab_contours(contours)\n",
        "  # get the max area contour\n",
        "  max_c = sorted(contours, key=cv.contourArea, reverse=True)[0]\n",
        "\n",
        "  # calculate the area of bounding box\n",
        "  bbox = cv.minAreaRect(max_c)\n",
        "  # the location of bbox\n",
        "  bbox = cv.cv.BoxPoints(bbox) if imutils.is_cv2() else cv2.boxPoints(bbox)\n",
        "  bbox = np.int0(box) # numpy method to convert bbox float point to int\n",
        "\n",
        "  # draw the contour on the img\n",
        "  cv.drawContour(img, bbox, -1, (0,0,255, 3))\n",
        "\n",
        "  cv2.imshow(\"Image\", img)\n",
        "  cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
