{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenCV_Tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPSAuugz0kzn4lXevHkUlPQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satorumi/Machine-Learning/blob/main/OpenCV_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7xfmovmvyIS"
      },
      "source": [
        "### Import Librabry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCxRNmeBw5nt",
        "outputId": "632dcfe3-59b3-40ee-a5f9-1744ec755531"
      },
      "source": [
        "!pip install mediapipe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.7/dist-packages (0.8.6)\n",
            "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.12.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.36.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJFJNlahvyU3"
      },
      "source": [
        "import cv2 as cv\n",
        "import mediapipe as mp\n",
        "import time\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e3A74ej4Jvu"
      },
      "source": [
        "### Section #1: Basics\n",
        "- Reading Images & Video\n",
        "- Resizing and Rescaling Frames\n",
        "- Drawing Shapes & Putting Text\n",
        "- 5 Essential Functions in OpenCV\n",
        "- Image Transformations\n",
        "- Contour Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Y59uxu1DpE"
      },
      "source": [
        "# a rescale function, work for images, videos\n",
        "def rescale(frame, scale=0.5): \n",
        "  width = int(frame.shape[1] * scale) # define frame width\n",
        "  height = int(frame.shape[0] * scale) # define frame height\n",
        "\n",
        "  dimension = (width, height)\n",
        "  return cv.resize(frame, dimensions, interpolation=cv.INTER_AREA) # using resize func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKBIg-aY28j4"
      },
      "source": [
        "# rescale frame, only work with live video\n",
        "def changeRes(width, height):\n",
        "  capture.set(3, width)\n",
        "  capture.set(4, height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VEFYJyr2acU"
      },
      "source": [
        "# display img\n",
        "img = cv.imread('img_path')\n",
        "resized_img= rescale(image)\n",
        "cv.imshow('Image', resized_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB3w8y0Aw96g"
      },
      "source": [
        "# Read in Video\n",
        "capture = cv2.VideoCapture(1) # int or video path, int specify comp camera\n",
        "\n",
        "mpHands = mp.solutions.hands\n",
        "hands = mpHands.Hands(static_image_mode=False) # keep all default parameters\n",
        "\n",
        "while True: # to read in video\n",
        "  isTrue, frame = cap.read() # succesfully read or not, frame in video\n",
        "  \n",
        "  resized_frame  = rescale(frame) # resize the frame\n",
        "  cv.imshow('Video', resized_frame) # display frame\n",
        "\n",
        "  if cv.waitKey(20) & 0xFF==ord('d'):  # if d is pressed\n",
        "    break # break out loop\n",
        "\n",
        "capture.release() # release capture instant\n",
        "cv.destroyAllWindows() # delete the video windows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXwKAhCZ35k7"
      },
      "source": [
        "# Create img, paint img\n",
        "blank_img = np.zeros(shape=(500, 500), dtype='uint8') # create blank image\n",
        "cv.imshow('Blank', blank_img)\n",
        "\n",
        "\n",
        "blank_img[:] = 0, 255, 0 # paint the img with green\n",
        "\n",
        "cv.imshow('Green', blank_img)\n",
        "\n",
        "blank_img[200:300, 300:400] = 0, 0 255 # pain a part of img with red\n",
        "cv.imshow('Red', blank_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ofpcm-F5myW"
      },
      "source": [
        "# Draw rectangle\n",
        "cv.rectangle(img=blank_img, pt1=(0,0), pt2=(255,500), color=(0, 0, 255), thickness=cv.FILLED # or an int)\n",
        "cv.imshow('Rectangle', blank_img)\n",
        "\n",
        "# draw circle\n",
        "\n",
        "cv.circle(blank_img, center=(blank_img.shape[1]//2, blank_img.shape[0]//2), radius=35, thickness=-1)\n",
        "cv.imshow('Circle', blank_img)\n",
        "\n",
        "# draw a line\n",
        "cv.line(blank_img, pt1=(0,0), pt2=(255,500), color=(0, 255, 0), thickness=3)\n",
        "cv.imshow('Line', blank_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N64j67Tz8Eb2"
      },
      "source": [
        "# write some text\n",
        "cv.putText(img=blank_img, text='OpenCV Beginner', org=(300,300), \n",
        "           fontFace=cv.FONT_HERSHEY_TRIPLEX, fontScale=1.0,\n",
        "           thickness=2, color=(0,0,255))\n",
        "cv.imshow('Text', blank_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9bvhQdJ8yYP"
      },
      "source": [
        "# covert color\n",
        "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) # convert to gray color\n",
        "\n",
        "# blur an image, there are different type of blur, ksize = blurness\n",
        "blur = cv.GaussianBlur(img, ksize=(3,3), cv.BORDER_DEFAULT)\n",
        "\n",
        "\n",
        "# edge cascade\n",
        "canny = cv.Canny(blur, threshold1=125, threshold2=175)\n",
        "\n",
        "# dilating image\n",
        "dilate - cv.dilate(canny, kernel=(3,3), iterations=2)\n",
        "\n",
        "# erode image\n",
        "erode = cv.erode(dilate, kernel=(3,3), iterations=2)\n",
        "\n",
        "resize_img = cv.resize(img, (500, 500), interpolation=cv.INTER_CUBIC)\n",
        "# crop by slicing\n",
        "crop_img = resize_img[:300, :300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATHN_-k8BDkl"
      },
      "source": [
        "# cv.warpAffine = applies an affine transformation to an image\n",
        "# translation\n",
        "def translate(img, x, y): # x-axis, y-axis\n",
        "  transMat = np.float32([[1, 0, x], [0, 1, y]])\n",
        "  dimensions = (img.shape[1], img.shape[0]) # width, height\n",
        "  return cv.warpAffine(image, transMat, dimensions) \n",
        "\n",
        "# rotation\n",
        "def rotate(img, angle=90, rot_point=None):\n",
        "  (width, height) = img.shape[:2]\n",
        "\n",
        "  if rot_point == None:\n",
        "    rot_point = (width//2, height//2)\n",
        "  \n",
        "  rotMat = cv.getRotationMatrix2D(rotPoint, angle, 1.0)\n",
        "  dimensions = (width, height)\n",
        "  return cv.wrapAffine(img, rotMat, dimensions)\n",
        "\n",
        "# Flipping\n",
        "flipped_img = cv.flip(img, flipcode=1) # 0 = horizontally, 1 = vertically, -1 = both\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho9luviOEsa7"
      },
      "source": [
        "# CONTOURS, a curve joining all the continuous points (along the boundary), \n",
        "# having same color or intensity\n",
        "imgray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) # obj should be white, background black\n",
        "ret, thresh = cv.threshold(imgray, 127, 255, 0) # using threshold\n",
        "contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE) # apply\n",
        "\n",
        "# draw contours\n",
        "blank = np.zeros(img.shape[:2], dtype='uint8')\n",
        "cv.drawContours(blank, contours, -1, color=(0,255,0), thickness=2) # draw all contours\n",
        "\n",
        "cnt = contours[4] # select the fourth contour\n",
        "cv.drawContours(img, [cnt], 0, (0,255,0), 3) # draw the fourth contour"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-MH4b4oRnL4"
      },
      "source": [
        "Noise-Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUdGMNVARnVz"
      },
      "source": [
        "# Denoise colored img\n",
        "img = cv.imread('img_path')\n",
        "seq = cv.imread('img_seq_path')\n",
        "denoised_img = cv.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)\n",
        "# for img sequences\n",
        "denoised_seq = cv.fastNlMeansDenoisingMulti(seq, 2, 5, None, 4, 7, 35)\n",
        "result = np.hstack(img, denoised_img, denoised_seq)\n",
        "cv.im_show(result)\n",
        "\n",
        "# Denoise gray img\n",
        "gray_img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "denoised_img = cv.fastNlMeansDenoising(img, None, 10, 7, 21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMHL0-wjISgr"
      },
      "source": [
        "### Section #2 - Advanced\n",
        "- Color Spaces / Color Extracting\n",
        "- BITWISE operations\n",
        "- Color Channels\n",
        "- Smoothing Image\n",
        "- Masking\n",
        "- Histogram Computation\n",
        "- Thresholding/Binarizing Images\n",
        "- Edge Detection\n",
        "- Template Matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoQUHpMBItEj"
      },
      "source": [
        "#### Color Spaces / Object Tracking\n",
        " - convert images from one color-space to another\n",
        " - extract a colored object 2ith masking and BITWISE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jmjCcdiIwMu"
      },
      "source": [
        "# convert images from one color-space to another\n",
        "hsv = cv.cvtColor(img, cv.BGR2HSV)\n",
        "lab = cv.cvtColor(img, cv.BGR2LAB)\n",
        "rgb = cv.cvtColor(img, cv.BGR2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnfk5MWSKRLd"
      },
      "source": [
        "# extract a color object from video\n",
        "cap = cv.VideoCapture(0)\n",
        "while True:\n",
        "  _, frame = cv.read()\n",
        "\n",
        "  hsv = cv.cvtColor(frame, cv.BGR2HSV) # conver image to HSV color spaces\n",
        "\n",
        "  # source: https://stackoverflow.com/questions/24892615/tracking-multiple-objects-by-color-opencv-2-x\n",
        "  # define colors layers\n",
        "  lower_blue = np.array([110,50,50])\n",
        "  upper_blue = np.array([130,255,255])\n",
        "\n",
        "  lower_green = np.array([50, 50, 120])\n",
        "  upper_green = np.array([70, 255, 255]) \n",
        "\n",
        "  # create mask to take colors from HSV image\n",
        "  green_mask = cv.inRange((hsv, lower_green, upper_green))\n",
        "  blue_mask = cv.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "  mask = green_mask + blue_mask\n",
        "  # Bitwise-AND mask and original image\n",
        "  # docs: https://docs.opencv.org/4.5.2/d2/de8/group__core__array.html#ga60b4d04b251ba5eb1392c34425497e14\n",
        "  result = cv.bitwise_and(frame, frame, mask=mask) # computes bitwise conjunction of the two array\n",
        "\n",
        "\n",
        "  cv.imshow('frame', frame) # original\n",
        "  cv.imshow('mask', mask) # the mask\n",
        "  cv.imshow('result', result) # extracted blue\n",
        "\n",
        "  k = cv2.waitKey(5) & 0xFF\n",
        "  if k == 27:\n",
        "      break\n",
        "cv.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC_F2o6hrl_s"
      },
      "source": [
        "#### BITWISE Operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE8qxBN2rout"
      },
      "source": [
        "blank = np.zeros((400, 400), dtypes='uint8')\n",
        "rectangle = cv.rectangle(blank.copy(), (30,30), (370,370), 255, -1) # filled rectangle \n",
        "circle = cv.circle(blank.copy(), (200,200), center=200, 255, -1) # filled circle\n",
        "bitwise_and(rectangle, circle) # find intersecting region\n",
        "bitwise_or(rectangle, circle) # find intersecting and non-intersecting regions\n",
        "bitwise_xor(rectangle, circle) # find non-intersecting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykIFrRBQjM9J"
      },
      "source": [
        "#### Color Channel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz1_-TzNjLJm"
      },
      "source": [
        "r, g, b = cv.split()\n",
        "img.shape() # height, width, color channel\n",
        "r.shape() # only height, width\n",
        "\n",
        "blank = np.zeros(shape=img.shape[:2], dtype='uint8')\n",
        "red = cv.merge([blank, blank, r]) # display red channel\n",
        "green = cv.merge([blank, g, blank]) # display green channel\n",
        "\n",
        "merged_image = cv.merge([r, g, b]) # merge together for original image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v_lC8q6lrj1"
      },
      "source": [
        "#### Smoothing\n",
        "- Blur images with various low pass filters\n",
        "- Apply custom-made filters to images (2D convolution)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmzW6wNTlqe6"
      },
      "source": [
        "# blur types\n",
        "avg_blur = cv.blur(img, (7,7))\n",
        "median_blur = cv.medianBlur(img, kernel=3)\n",
        "gauss_blur = cv.GaussianBlur(img,(5,5),0)\n",
        "\n",
        "blur = cv.bilateralFilter(img,9,75,75) # noise removal while keeping edges sharp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fTg-M0mu2Hy"
      },
      "source": [
        "#### Histogram\n",
        "- Find histograms, using both OpenCV and Numpy functions\n",
        "- Plot histograms, using OpenCV and Matplotlib functions\n",
        "- histogram equalization and use it to improve the contrast of our images\n",
        "-  find and plot 2D color histograms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRZldHaSpCqC"
      },
      "source": [
        "for index, color in enumerate(['b', 'g', 'r']): # loop through all color channell\n",
        "  # calchist with each color channel \n",
        "  hist = cv.calcHist([img],channels=[index], None, histSize=[256], ranges=[0,256])\n",
        "  plt.plot(hist, color=color)\n",
        "  plt.xlim([0, 256])\n",
        "plt.show()\n",
        "\n",
        "mask = np.zeros(img.shape[:2], np.uint8)\n",
        "mask[100:300, 100:400] = 255\n",
        "hist_mask = cv.calcHist([img],[0],mask,[256],[0,256]) # added a mask\n",
        "\n",
        "# hist,bins = np.histogram(img.ravel(),256,[0,256]) using numpy\n",
        "\n",
        "plt.plot(hist_mask)\n",
        "plt.xlim([0, 256])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Wx0J914Ktl"
      },
      "source": [
        "# improve background contrast with equalize function\n",
        "equalize_img = cv.equalizeHist(img) # input grayscale image, output histogram equalized image\n",
        "result = np.hstack((img, equalize_img)) # stacking images side-by-side\n",
        "cv.imwrite('result img', result)\n",
        "\n",
        "# CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
        "# docs: https://docs.opencv.org/4.5.2/d5/daf/tutorial_py_histogram_equalization.html\n",
        "clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "cl1 = clahe.apply(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auDPusudA8II"
      },
      "source": [
        "##### 2D Color map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynh1tPBe-oAl"
      },
      "source": [
        "# source = https://www.programmersought.com/article/9447734765/\n",
        "\n",
        "# define a callback func\n",
        "def set_scale(val):\n",
        "    global hist_scale\n",
        "    hist_scale = val\n",
        "\n",
        "# Build HSV color map\n",
        "hsv_map = np.zeros((180, 256, 3), dtype=np.uint8)\n",
        "h, s = np.indices(hsv_map.shape[:2])\n",
        "hsv_map[:, :, 0] = h\n",
        "hsv_map[:, :, 1] = s\n",
        "hsv_map[:, :, 2] = 255\n",
        "hsv_map = cv.cvtColor(hsv_map, cv2.COLOR_HSV2BGR)\n",
        "cv.imshow('hsv map', hsv_map)\n",
        "\n",
        "cv.namedWindow('color hist', 0)\n",
        "hist_scale = 10\n",
        "cv.createTrackbar('scale', 'color hist', hist_scale, 32, set_scale)\n",
        "\n",
        "\n",
        "while True:\n",
        "    img = cv.imread('test1.jpg')\n",
        "    cv.imshow('image', img)\n",
        "      # Reduce resolution by image pyramid, but it does not affect the histogram too much.\n",
        "      # But this low resolution can suppress noise very much, thus removing the influence of isolated small dots on the histogram\n",
        "    small = cv.pyrDown(frame)\n",
        "    hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
        "      # Take the value of v channel (brightness).\n",
        "    dark_mask = hsv[:, :, 2] < 32\n",
        "    hsv[dark_mask] = 0\n",
        "    h = cv.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
        "    h = np.clip(h * 0.005 * hist_scale, 0, 1)\n",
        "    vis = hsv_map * h[:, :, np.newaxis] / 255.0\n",
        "    cv.imshow('color hist', vis)\n",
        " \n",
        "    ch = 0xFF & cv.waitKey(1)\n",
        "    if ch == 27:\n",
        "        break\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHwDDVVS_t8h"
      },
      "source": [
        "##### Histogram Backprojection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdXJ2H3gO0WE"
      },
      "source": [
        "# covert color to HSV\n",
        "img = cv.imread('rose_red.png')\n",
        "img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
        "\n",
        "target = cv.imread('rose.png')\n",
        "hsv_target = cv.cvtColor(target, cv.COLOR_BGR2HSV)\n",
        "\n",
        "# calc hist\n",
        "img_hist = cv.calcHist([img_hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
        "\n",
        "# normalize histogram and apply backprojection\n",
        "cv.normalize(img_hist, img_hist, 0 , 255, cv.NORM_MINMAX)\n",
        "dst = cv.calcBackProject([hsv_target], [0, 1], img_hist, [0, 180 , 0, 256], 1)\n",
        "\n",
        "# Now convolute with circular disc\n",
        "disc = cv.getStructuringElement(cv.MORPH_ELLIPSE, (5,5))\n",
        "cv.filter2D(dst, -1, disc, dst)\n",
        "\n",
        "# threshold \n",
        "threshold, thresh = cv.threshold(dst, 50, 255, 0)\n",
        "thresh = cv.merge((thresh, thresh, thresh)) # merge thresh_img\n",
        "result = cv.bitwise_and(target, thresh) # tak intersect of target and thresh\n",
        "\n",
        "result = np.vstack((target, thresh, result)) # stack original, thresh_img and final result\n",
        "cv.imwrite('result', result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8PJ1d778DQM"
      },
      "source": [
        "#### Thresholding / Binarizing\n",
        "Thresholding: set values smaller than the threshold, it is set to 0, otherwise values will be set to a maximum value. Produce a binary image where pixels are either white or black\n",
        "\n",
        "[OpenCV docs: Thresholding](https://docs.opencv.org/4.5.2/d7/d4d/tutorial_py_thresholding.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNTfyg_nJ4Ge"
      },
      "source": [
        "Simple Thresholding: using a global value as the threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W4QJpqb8cFe"
      },
      "source": [
        "# source image have to be a gray scale image\n",
        "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "# return a threshold that was used and the thresholded image\n",
        "threshold1, thresh1 = cv.threshold(gray, 127, 255, cv.THRESH_BINARY)\n",
        "threshold2, thresh2 = cv.threshold(gray, 127, 255 cv.THRESH_BINARY_INV)\n",
        "threshold3, thresh3 = cv.threshold(gray, 127, 255, cv.THRESH_TRUNC)\n",
        "threshold4, thresh4 = cv.threshold(gray, 127, 255, cv.THRESH_TOZERO)\n",
        "threshold5, thresh5 = cv.threshold(gray, 127, 255, cv.THRESH_TOZERO_INV)\n",
        "\n",
        "titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
        "images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
        "for i in range(6):\n",
        "    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray',vmin=0,vmax=255)\n",
        "    plt.title(titles[i])\n",
        "    plt.xticks([]),plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x52QaRhBJxEl"
      },
      "source": [
        "##### Adaptive Thresholding\n",
        "- determines the threshold for a pixel based on a small region around it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3nwErhHJuJA"
      },
      "source": [
        "thresh6 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\\\n",
        "            cv.THRESH_BINARY,11,2)\n",
        "thresh7 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
        "            cv.THRESH_BINARY,11,2)\n",
        "\n",
        "titles = ['Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
        "images = [thresh6, thresh7]\n",
        "\n",
        "for images in range(len(images)):\n",
        "  plt.plot(1, 2, i+1), plt.imshow(images[image])\n",
        "  plt.title = titles[image]\n",
        "  plt.xticks([]),plt.yticks([])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17RmSCRgM3HE"
      },
      "source": [
        "Otsu's Threshold -  determines an optimal global threshold value from the image histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGbnJkWLM2rg"
      },
      "source": [
        "# otsu threshold is passed as an extra flag\n",
        "threshold8, thresh8 = cv.threshold(blur,0,255,cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
        "\n",
        "# otsu threshold after a blurr layer\n",
        "blur = cv.GaussianBlur(img,(5,5),0)\n",
        "threshold9, thresh9 = cv.threshold(blur,0,255,cv.THRESH_BINARY + cv.THRESH_OTSU()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QKXyfSXSUd1"
      },
      "source": [
        "#### Edge-Detection / Image Gradients\n",
        "- Find Image gradients, edges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-4aBFv2SYt7"
      },
      "source": [
        "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "canny = cv.Canny(gray, 150, 175)\n",
        "\n",
        "lap = cv.Laplacian(gray, cv.CV_64F)\n",
        "lap = np.uint8(np.absolute(lap))\n",
        "\n",
        "sobelx = cv.Sovel(gray, cv.CV_64F, 1, 0) # compute with x-axis\n",
        "sobely = cv.Sovel(gray, cv.CV_64F, 0, 1) # compute with y-axis\n",
        "combined_sobel = cv.bitwise_or(sobelx, sobely)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pcDByXQhcSA"
      },
      "source": [
        "#### Template Matching\n",
        "- find objects in an image using Template Matching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73LNrIdCoH59"
      },
      "source": [
        "Single Object with different methods\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjnY4K6dkJAB"
      },
      "source": [
        "img = cv.imread('img.jpg', 0)\n",
        "img2 = img.copy()\n",
        "template = cv.imread('template.jpg', 0)\n",
        "h, w = template.shape[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "RHuW4Kd8lJ3S",
        "outputId": "29509cb3-d625-455b-f70e-7c79a50ee217"
      },
      "source": [
        "# All the 6 methods for comparison in a list\n",
        "methods = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR',\n",
        "            'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED']\n",
        "\n",
        "for m in methods:\n",
        "  img = img2.copy()\n",
        "  method = eval(m)\n",
        "\n",
        "  # Apply template Matching\n",
        "  matched = cv.matchTemplate(img, template, method) # returns a grayscale image, pixel denotes how much pixel match with template  \n",
        "  min_val, max_val, min_loc, max_loc = cv.MinMaxLoc(matched) #  find where is the maximum/minimum value\n",
        "\n",
        "  \n",
        "  # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
        "  if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]: \n",
        "    top_left = min_loc\n",
        "  else:\n",
        "    top_left = max_loc\n",
        "\n",
        "  bottom_right = (top_left[0] + w, top_left[1] + h) #  output image will have a size of (W-w+1, H-h+1)\n",
        "\n",
        "  # rectangle is your region of template\n",
        "  cv.rectangle(img, top_left, bottom_right, 255, 2)\n",
        "\n",
        "  plt.subplot(121),plt.imshow(matched, cmap = 'gray')\n",
        "  plt.title('Matching Result'), plt.xticks([]), plt.yticks([])    \n",
        "  plt.subplot(122),plt.imshow(img, cmap = 'gray')\n",
        "  plt.title('Detected Point'), plt.xticks([]), plt.yticks([])\n",
        "  plt.subtitle(m)\n",
        "     \n",
        "plt.show()      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-af4342fc6491>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKVI_OhmoMMr"
      },
      "source": [
        "Multiple objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coqay6StoNqy"
      },
      "source": [
        "img = cv.imread('image', 0)\n",
        "img_gray = cv.cvtColor(img, cv.BGR2GRAY) # turn to gray image\n",
        "template = cv.imread('template', 0)\n",
        "w, h - template.shape[::-1]\n",
        "\n",
        "# apply matching\n",
        "matched = cv.matchTemplate(img_gray, template, cv.TM_CCOEFF_NORMED)\n",
        "\n",
        "threshold = 0.8 # use threshold to detect all location\n",
        "locations = np.where(matched >= threshold)\n",
        "for location in zip(*locations[::-1]): # loop through every location detected in\n",
        "   cv.rectangle(img, location, (location[0] + w, location[1] + h), color=(0,0,255), 1) # draw a red rectangle\n",
        "\n",
        "cv.imwrite('result', img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pTJmG44rdVo"
      },
      "source": [
        "#### Image Pyramid\n",
        "- learn about Image Pyramids\n",
        "- use Image pyramids to create a new fruit, \"Orapple\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0s4HiwhrfyX"
      },
      "source": [
        "img = cv.imread('image.jpg')\n",
        "lower_resolution = cv.pyrDown(higher_resolution) # Blurs an image and downsamples it, from higher -> lower\n",
        "higher_resolution = cv.pyrUp(lower_resolution) # from lower to higher, can not convert back to orignal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0XLyCbVtsj9"
      },
      "source": [
        "# use Image pyramids to create a new fruit, \"Orapple\"\n",
        "\n",
        "# Gaussian pyramids can be formed by using cv.pyrDown() and cv.pyrUp()\n",
        "# Laplacian Pyramids are formed from the Gaussian Pyramids\n",
        "\n",
        "apple = cv.imread('apple.jpg')\n",
        "orange = cv.imread('orange.jpg')\n",
        "\n",
        "a = apple.copy()\n",
        "o = orange.copy()\n",
        "\n",
        "apple_gp = [a]\n",
        "orange_gp = [o]\n",
        "\n",
        "for i in range(6): # create 6 levels Gaussian pyramid for apple and orange\n",
        "  lower_a = cv.pyrDown(a)\n",
        "  apple_gp.append(lower_a)\n",
        "  lower_o = cv.pyrDown(o)\n",
        "  orange_gp.append(lower_o)\n",
        "\n",
        "# generate Laplacian Pyramid for apple and orange\n",
        "apple_lp = [apple_gp[5]]\n",
        "orange_lp = [orange_gp[5]]\n",
        "\n",
        "for i in range(5, 0, -1):\n",
        "    GE = cv.pyrUp(apple_gp[i]) # apple\n",
        "    L = cv.subtract(apple_gp[i-1], GE) # find different\n",
        "    apple_lp.append(L)\n",
        "\n",
        "    GE = cv.pyrUp(orange_gp[i]) # orange\n",
        "    L = cv.subtract(orange_gp[i-1], GE)\n",
        "    orange_lp.append(L)\n",
        "\n",
        "\n",
        "\n",
        "# Now add left and right halves of images in each level\n",
        "LS = []\n",
        "for la, lo in zip(apple_lp, orange_lp):\n",
        "    rows, cols, dpt = la.shape\n",
        "    ls = np.hstack((la[:, 0:cols/2], lo[:,cols/2:])) # stack horizontaly\n",
        "    LS.append(ls)\n",
        "\n",
        "# now reconstruct\n",
        "ls_ = LS[0]\n",
        "for i in range(1, 6):\n",
        "    ls_ = cv.pyrUp(ls_)\n",
        "    ls_ = cv.add(ls_, LS[i])\n",
        "\n",
        "# image with direct connecting each half\n",
        "real = np.hstack((apple[:, :cols/2], orange[:, cols/2:]))\n",
        "cv.imwrite('Pyramid_blending2.jpg', ls_)\n",
        "cv.imwrite('Direct_blending.jpg', real)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmxSNYMaWFir"
      },
      "source": [
        "#####Shape Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B6QBZb_WJk6"
      },
      "source": [
        "shape_img = cv.imread('shape_img_path')\n",
        "blank_img = np.zeros(np.uint8)\n",
        "gray_img = cv.cvtColor(shape, cv.COLOR_BGR2GRAY) #convert to gray color\n",
        "# create threshold, return img with all shapes as black\n",
        "_, thresh = cv.threshold(gray_img, 50, 255, 1)\n",
        "contours, hierarchy = cv.findContours(thresh, 1, 2)\n",
        "\n",
        "for contour in contours:\n",
        "  # approxPolyDP return approx location of vertices in shape\n",
        "  approx_points = contour.approxPolyDP(contour, \n",
        "                  epsilon=0.01*cv.arcLength(countour),closed=True))\n",
        "  if len(approx_points) == 4: # a sqaure\n",
        "    # draw contours\n",
        "    cv.drawContours(shape_img, [contour], 0, 255, 10)\n",
        "  elif len(approx_points) == 3:\n",
        "    cv.drawContours(shape_img, [contour], 0, (0,0,255), 10)\n",
        "\n",
        " \n",
        "cv.imshow(shape_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG_EJgeRca62"
      },
      "source": [
        "#####Ball Tracking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hltoyfbZa6F-"
      },
      "source": [
        "cap = cv.VideoCapture('ball_video_path')\n",
        "ball = [] # keep track of ball locations\n",
        "# create an output vid\n",
        "output_vid = cv.VideoWriter('video_name', cv.VideoWriter_fourcc('M', 'J', 'P', 'G'), 10, (1920, 1080))\n",
        "while cap.isOpened():\n",
        "  _, frame = cap.read()\n",
        "  hsv = cv.cvtColor()\n",
        "  # using color of the ball to create mask\n",
        "  upper_ball_color = np.array([21,0,0])\n",
        "  lower_ball_color = np.array([45,255,255])\n",
        "  # create mask by 2 color\n",
        "  color_mask = cv.inRange(hsv, upper_ball_color, lower_ball_color)\n",
        "\n",
        "  # use mask to find contours\n",
        "  contours, _ = cv.findContour(color_mask, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
        "  if contours: # found contours\n",
        "    max_c = max(contours, key-cv.contourArea) # get the max area contour\n",
        "    # finds a circle of the minimum area enclosing a 2D point set\n",
        "    # return the center and radius\n",
        "    ((x, y) radius)) = cv.minEnclosingCircle(max_c)\n",
        "    m = cv.moments(max_c)\n",
        "    try: # x coordinate, y coordiante\n",
        "      center = (int(m['m10']/m['m00']), int(m['m01']/m['m00']))\n",
        "      cv.circle(frame, center, 10, (255,0,0), -1)\n",
        "      ball.append(center) # add center of tracked ball\n",
        "    except:\n",
        "      pass\n",
        "    if len(ball) >= 2:\n",
        "      for i in range(1, len(ball)):\n",
        "        # draw line with previous and current ball locations\n",
        "        cv.line(frame, ball[i-1], ball[i], (255,0,0),5) \n",
        "  output_vid.write(frame)\n",
        "output_vid.release()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
